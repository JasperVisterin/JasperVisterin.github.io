<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Controller Optimization Animations</title>

  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body {
      font-family: sans-serif;
      margin: 40px;
      max-width: 900px;
    }
    .video-container {
      margin: 20px 0;
    }
    .math {
      margin-top: 20px;
      font-size: 1.2em;
    }
  </style>
</head>
<body>

  <h1>Distributed Delay Controller Optimization</h1>

  <p class="math">
    The optimization framework minimizes the spectral abscissa:  
    $$\min_{\mathbf{p}} \max \{\operatorname{Re}(\lambda)\} = \min_{\mathbf{p}} \sigma_{\max}$$  
    where \( \lambda \) are the characteristic roots of the plant \(P\) depending on controller parameters \( \mathbf{p} \). In this case, the Distributed Delay Controller is studied, and \(K_1, K_2\) and \(c\) make up the parameters \(\mathbf{p}\).
    $$ C \leftrightarrow u(t)=K_{1} y(t) + K_{2} \int_{-\tau}^{0} g(\theta) y(t+\theta) d \theta  $$
    $$ 
    g(\theta)=\sum_{k=0}^K c_k T_k\left(2 \frac{\theta}{\tau}+1\right)
    $$ 
  </p>

 <p class="math">
    The main concern when doing this is that the kernel \(g(\theta)\) will exhibit extreme oscillations when \( K \) is chosen large. This effect has all kinds of undesired consequences. Hence,
    two approaches have been developed to deal with this. On one hand, there is a Penalty-Based Regularization which uses a Penalty-term \(REG(c)\). This Penalty-term is implemented in the following way:
    $$
   REG(c)=R[g]=\int_{-\tau}^{0}\left(g^{\prime}(\theta)\right)^2 d \theta
   $$
   $$
    \min_{\mathbf{p}} \max \{\operatorname{Re}(\lambda)\} = \min_{\mathbf{p}} \sigma_{\max}
   $$
   The other option is a more heuristic regularization. This heuristic, which is called the Damping Heuristic, has been developed via Chebyshev approximation theory. 
   The intuition used here is that, roughly speaking, for analytic functions that are analytic on a larger domain that these functions have Chebyshev coefficients that decay faster than functions which are analytic on a smaller domain. Assuming reasonable conditions, both functions have coefficients that decay exponentially [1]. 
   Hence, to obtain smooth kernels \(g(\theta)\), this principle can be employed. The Chebyshev coefficients are damped with an exponential factor \(\rho>1)\: 
   $$
   c_k= 
\begin{cases}
\tilde{c}_k& 0\leq k \leq 5 \\
\frac{\tilde{c}_k}{\rho^{k-5}}, & 6 \leq k
\end{cases}    
   $$
   Note that this concept does not directly penalize the global smoothness, but controls the kernel's resolution to avoid overfitting or instability. It acts as a practical damping mechanism during optimization. 
   While \(\rho)\ is still a hyperparameter, the concept is simple, interpretable, and rooted in Chebyshev approximation theory. 
 </p>
  
  <p class="math">
    The videos below show the evolution of the spectrum as parameters are tuned for the following system:
    $$ P \leftrightarrow  \dot{x}(t) = (1-e^{-1})x(t)+\frac{1}{2}x(t-1)+\frac{e}{2}\,x(t-2)+\frac{e^2}{2} x(t-3) +\int_{-1}^{0}\theta \,x(t+\theta)d\theta  + u\left(t-\frac{1}{32}\right) $$
  </p>

    <div class="video-container">
    <video width="100%" controls>
      <source src="first_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

    <div class="video-container">
    <video width="100%" controls>
      <source src="second_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

    <p class="math">
    The video below shows the evolution of the spectrum as parameters are tuned for the following system:
    $$ P \leftrightarrow   \dot{x}(t) = \left(1-e^{-1}\right)\begin{bmatrix}
1 & 2 \\
2 & 1 \\
\end{bmatrix}x(t)
+\frac{1}{2}\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}x(t-1)+
\frac{e}{2}\begin{bmatrix}
-2 & 1 \\
1 & -2 \\
\end{bmatrix}x(t-2)+ \int_{-1}^{0}\begin{bmatrix}
2e^{\theta} & 0 \\
0 & e^{\theta} \\
\end{bmatrix} x(t+\theta)
 d\theta 
 +
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}u\left(t-\frac{1}{2}\right)  $$
  </p>

  <div class="video-container">
    <video width="100%" controls>
      <source src="system_2_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>



   <p class="math">
     Sources:
    [1] L. N. Trefethen, Approximation Theory and Approximation Practice, Extended Edition. 1 2019.
   </p>
  
</body>
</html>
