<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Controller Optimization Animations</title>

  <!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body {
      font-family: sans-serif;
      margin: 40px;
      max-width: 900px;
    }
    .video-container {
      margin: 20px 0;
    }
    .math {
      margin-top: 20px;
      font-size: 1.2em;
    }
  </style>
</head>
<body>

  <h1>Distributed Delay Controller Optimization</h1>

  <p class="math">
    This website was created to support the Thesis 'Control by Shaping Delay Distribution' by Jasper Visterin. If you are already familiar with the matter, 
    feel free to skip to the animations illustrating a tradeoff between kernel smoothness and closed-loop plant performance, or more concretely, the regularization-spectral abscissa tradeoff.
  </p>

 <p class="math">
    The main concern when doing this is that the kernel \(g(\theta)\) will exhibit extreme oscillations when \( K \) is chosen large. This effect has all kinds of undesired consequences. Hence,
    two approaches have been developed to deal with this. On one hand, there is a Penalty-Based Regularization which uses a Penalty-term \(REG(c)\). This Penalty-term is implemented in the following way:
    $$
   REG(c)=R[g]=\int_{-\tau}^{0}\left(g^{\prime}(\theta)\right)^2 d \theta
   $$
   $$
    \min_{\mathbf{p}} \max \{\operatorname{Re}(\lambda)\} = \min_{\mathbf{p}} \sigma_{\max}
   $$
   The other option is a more heuristic regularization. This heuristic, which is called the Damping Heuristic, has been developed via Chebyshev approximation theory. 
   The intuition used here is that, roughly speaking, for analytic functions that are analytic on a larger domain that these functions have Chebyshev coefficients that decay faster than functions which are analytic on a smaller domain. Assuming reasonable conditions, both functions have coefficients that decay exponentially [1]. 
   Hence, to obtain smooth kernels \(g(\theta)\), this principle can be employed. The Chebyshev coefficients are damped with an exponential factor \( \rho>1 \): 
   $$
   c_k= 
\begin{cases}
\tilde{c}_k& 0\leq k \leq 5 \\
\frac{\tilde{c}_k}{\rho^{k-5}}, & 6 \leq k
\end{cases}    
   $$
   Note that this concept does not directly penalize the global smoothness, but controls the kernel's resolution to avoid overfitting or instability. It acts as a practical damping mechanism during optimization. 
   While \( \rho \) is still a hyperparameter, the concept is simple, interpretable, and rooted in Chebyshev approximation theory. 
 </p>

  <p class="math">
    Furthermore, as regularization is applied, the spectral abscissa will always experience an increase compared to the situation where \(g(\theta)\) is not regularized. 
    It is, in fact, a multi-objective optimization problem where two objectives are optimized at once. 
    In bi-objective optimization, different linear combinations of two convex objective functions, \(J_1(\mathbf{p}) \) and \(J_2(\mathbf{p}) \), trace out the Pareto front. 
    However, for the two objectives \(\sigma_{\max}\) and \(REG(c)\), the complete objective function will be non-convex due to the presence of the spectral abscissa. 
    Hence, this visualization is only valid for a specific local minimum. In the non-convex case, 
    constructing a Pareto front requires identifying a local minimum of the weighted objective for a given \( \alpha \), 
    and then repeatedly resolving the optimization problem over a grid of \( \alpha  \) values. This process aims to track how the local minimum evolves as $\alpha$ varies, attempting to trace the local Pareto front.
$$
 \min _{\mathbf{p}} \alpha J_1(\mathbf{p})+(1-\alpha)J_2(\mathbf{p}),\quad\alpha\in[0,1]
$$
Experiments illustrated that it is very difficult to obtain a proper Pareto front even with intense sampling. 
    Occasional jumps to different local minima still occur, even on intensively sampled grids. A possible explanation for this is the high sensitivity of the spectral abscissa. The purpose of this website is to show
  how this Pareto Front is formed using the different kinds of regularization. In the case of the Damping Heuristic, one cannot strictly speak about a bi-objective optimization because the damping is implicit unlike \( REG(c) \).
    However, the visual of the Pareto Front can also be obtained by using damping. As you continue on this website, you will see in the provided animations 
    how the spectral abscissa changes when a regularization parameter like \( \alpha \) or \( \rho \) is changed. 
  </p>
  
  <p class="math">
    The videos below show the evolution of the spectrum as parameters are tuned for the following system:
    $$ P \leftrightarrow  \dot{x}(t) = (1-e^{-1})x(t)+\frac{1}{2}x(t-1)+\frac{e}{2}\,x(t-2)+\frac{e^2}{2} x(t-3) +\int_{-1}^{0}\theta \,x(t+\theta)d\theta  + u\left(t-\frac{1}{32}\right) $$
    For this system, illustrations of both Penalty- as Damping-regularization are given.
  </p>

    <div class="video-container">
    <video width="100%" controls>
      <source src="first_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

    <div class="video-container">
    <video width="100%" controls>
      <source src="second_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

      <div class="video-container">
    <video width="100%" controls>
      <source src="system_1_alpha_animation.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

    <p class="math">
    The video below shows the evolution of the spectrum as parameters are tuned for the following system:
    $$ P \leftrightarrow   \dot{x}(t) = \left(1-e^{-1}\right)\begin{bmatrix}
1 & 2 \\
2 & 1 \\
\end{bmatrix}x(t)
+\frac{1}{2}\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}x(t-1)+
\frac{e}{2}\begin{bmatrix}
-2 & 1 \\
1 & -2 \\
\end{bmatrix}x(t-2)+ \int_{-1}^{0}\begin{bmatrix}
2e^{\theta} & 0 \\
0 & e^{\theta} \\
\end{bmatrix} x(t+\theta)
 d\theta 
 +
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}u\left(t-\frac{1}{2}\right)  $$
  </p>

  <div class="video-container">
    <video width="100%" controls>
      <source src="system_2_experiment.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>



   <p class="math">
     Sources: [1] L. N. Trefethen, Approximation Theory and Approximation Practice, Extended Edition. 1 2019.
   </p>
  
</body>
</html>
